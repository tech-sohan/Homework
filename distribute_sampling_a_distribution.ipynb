{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "distribute_sampling_a_distribution.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tech-sohan/Homework/blob/main/distribute_sampling_a_distribution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9rUcOaIgv9Z"
      },
      "source": [
        "# HW: Probability, Sampling, and the Central Limit Theorem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn3b9geCgv9c"
      },
      "source": [
        "Suppose you were to flip a coin. Then you expect not to be able to say whether the next toss would yield a heads or a tails.  You might tell a friend that the odds of getting a heads is equal to to the odds of getting a tails, and that both are $1/2$.\n",
        "\n",
        "This intuitive notion of odds is a **probability**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TbpCfnxgv9e"
      },
      "source": [
        "## Probability as symmetry, or from a model\n",
        "\n",
        "### Symmetry\n",
        "\n",
        "Consider another example. If we were tossing a 'fair' six-sided dice, we may thus equivalently say that the odds of the dice falling on any one of its sides is $1/6$. Indeed if there are $C$ different equally likely possibilities, we'd expect that the probability of any one particular outcome would be $1/C$.\n",
        "\n",
        "The examples of the coin as well as the dice illustrate the notion of probability springing from **symmetry**. Here we think of probability of of the number 4 on the dice as the ratio:\n",
        "\n",
        "$$\\frac{Number\\: of\\: cases\\: for\\: number\\: 4}{number\\: of\\: possibilities} = \\frac{1}{6},$$\n",
        " assuming equally likely possibilities.\n",
        "\n",
        "\n",
        "\n",
        "### From a model\n",
        "\n",
        "But now think of an event like an election, say a presidential election. You cant exactly run multiple trials of the election: its a one-off event. But you still want to talk about the likelihood of a candidate winning. However people do make **models** of elections, based on inputs such as race, age, income, sampling polls, etc. They assign likeyhoods of candidates winning and run large numbers of **simulations** of the election, making predictions based on that. \n",
        "\n",
        "Or consider what a weather forecaster means when he or she says there is a 90% chance of rain today. Presumably, this conclusion has been made from many computer **simulations** which take in the weather conditions known in the past, and propagated using physics to the current day. The simulations give different results based on the uncertainty in the measurement of past weather, and the inability of the physics to capture the phenomenon exactly (all physics is some approximation to the natural world). But 90% of these simulations show rain.\n",
        "\n",
        "In all of these cases, there is either a model (a fair coin, an election forecasting model, a weather differential equation), or an experiment ( a large number of coin tosses) that is used to **estimate** a probability, or the odds, of an **event** $E$ occuring. \n",
        "\n",
        "## Probability as frequency\n",
        "\n",
        "The example above of doing multiple symbols has the feel of defining probability in terms of frequency, even if the frequency is in terms of simulations run in code on a computer.\n",
        "\n",
        "Consider doing a large number of coin flips. You would do, or imagine doing, a large number of flips or **trials** $N$, and finding the number of times you got heads $N_H$. Then the probability of getting heads would be \n",
        "$$\\frac{N_H}{N}.$$\n",
        "\n",
        "This is the notion of probability as a **relative frequency**: if there are multiple ways an **event** like the tossing of a coin can happen, lets look at multiple trials of the event and see the fraction of times one or other of these ways happened. \n",
        "\n",
        "This jibes with our general notion of probability from symmetry: indeed you can think of it as an experimental verification of a symmetry based model.\n",
        "\n",
        "We can test the model of a fair coin by having carried out a large number of coin flips. You would do, or imagine doing, a large number of flips or **trials** $N$, and finding the number of times you got heads $N_H$. Then the probability of getting heads would be \n",
        "$$\\frac{N_H}{N}.$$\n",
        "\n",
        "### Q1. Simulating the results of the model\n",
        "\n",
        "We dont have a coin right now. So let us **simulate** this process on a computer. To do this we will use a form of the **random number generator** built into `numpy`. In particular, we will use the function `np.random.choice`, which will with equal probability for all items pick an item from a list (thus if the list is of size 6, it will pick one of the six list items each time, with a probability 1/6).\n",
        "\n",
        "Implement a function `throw_a_coin(N)` which returns a sequence of N coin tosses, with each toss either being a 'H' or a 'T' (with equal probability). So a sequence of 40 tosses may look like:\n",
        "\n",
        "```\n",
        "T T H H T T T H T H T H T T H T H T T H H T T H T T H T T H H H H T H T H H T T\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RrB7lRsgv9g"
      },
      "source": [
        "import numpy as np\n",
        "def throw_a_coin(N):\n",
        "    np.random.choice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2n-iO6Hgv9h"
      },
      "source": [
        "We use your code here to make 40 coin tosses:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX7F7WiJgv9i"
      },
      "source": [
        "throws=throw_a_coin(40)\n",
        "print(\"Throws:\",\" \".join(throws))\n",
        "print(\"Number of Heads:\", np.sum(throws=='H'))\n",
        "print(\"p1 = Number of Heads/Total Throws:\", np.sum(throws=='H')/40.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxT7Li9wgv9i"
      },
      "source": [
        "Notice that you do not necessarily get 20 heads.\n",
        "\n",
        "Now say that we run the entire process again, a second **replication** to obtain a second sample. Then we ask the same question: what is the fraction of heads we get this time? Lets call the odds of heads in sample 2, then, $p_2$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W0yOP1xgv9j"
      },
      "source": [
        "def make_throws(N):\n",
        "    throws=throw_a_coin(N)\n",
        "    return np.sum(throws=='H')/N # or np.mean(throws=='H')\n",
        "make_throws(40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzZuJ6y4gv9k"
      },
      "source": [
        "Our intuitive notion is that as we do many more trials, we should find half the tosses being heads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtz1vEyCgv9k"
      },
      "source": [
        "make_throws(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxQIzbrXgv9l"
      },
      "source": [
        "make_throws(10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZNy4lVlgv9l"
      },
      "source": [
        "As you can see, the larger number of trials we do, the closer we seem to get to half the tosses showing up heads. Lets see this more systematically by making a matplotlib plot. Construct an array of trials from 0 to 400000 in steps of 1000:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUfb8g13gv9m"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "trials=np.arange(0, 400000, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHK5F5Opgv9n"
      },
      "source": [
        "### Q2: Plot the fraction of heads against the number of trials. Plot a horizontal red line at 0.5\n",
        "\n",
        "For each entry in trials, call `make_throws` and plot the resulting array of fractions (lets call this array `trials_fractions`) against the trials array.\n",
        "\n",
        "We are assuming H and T are equally likely, so the probability by symmetry is 1/2, and we wantt to see how this probability is approached in the long run of \"infinite\" coin tosses.\n",
        "\n",
        "Make sure you label your axes and title your graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRecBkDLgv9n"
      },
      "source": [
        "trials_fractions = [make_throws(j) for j in trials]\n",
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSEvCft4gv9p"
      },
      "source": [
        "You will see, the true odds **fluctuate** about their long-run value of 0.5, in accordance with the model of a fair coin (which we encoded in our simulation by having `np.random.choice` choose between two possibilities with equal probability), with the fluctuations becoming much smaller. These **fluctations** are what give rise to probability distributions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWk2k5Dcgv9q"
      },
      "source": [
        "## The law of large numbers\n",
        "\n",
        "This convergence of the fractions to the value of 0.5 is called the law of large numbers. The 0.5 value is the mean of a \"Bernoulli\" Distribution, the distribuion for *one* coin toss. The fractions are really running averages:\n",
        "\n",
        "$$ S_n = \\frac{1}{n}\\sum_{i=1}^{n} x_i $$\n",
        "\n",
        "where x_i is either H or T (1 or 0).\n",
        "\n",
        "### The Discrete Bernoulli Distribution\n",
        "\n",
        "The distribution for 1 `coin toss` is called the Bernoulli.\n",
        "\n",
        "We have been using this so far both in elecctions and in coin-tosses withour talking about it formally.\n",
        "\n",
        "Say a coin flip represented as $X$, where $X=1$ is heads, and $X=0$ is tails. The parameter is probability of heads $p$.\n",
        "\n",
        "$$X \\sim Bernoulli(p)$$\n",
        "\n",
        "is to be read as $$X$$ **has distribution** $Bernoulli(p)$.\n",
        "\n",
        "\n",
        "Bernoulli pmf:\n",
        "\n",
        "$$\n",
        "f(x) =\n",
        "\\begin{cases}\n",
        "1 - p & x = 0\\\\\n",
        "p & x = 1.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "for p in the range 0 to 1.\n",
        "\n",
        "$$f(x) = p^x (1-p)^{1-x}$$\n",
        "\n",
        "for x in the set {0,1}.\n",
        "\n",
        "On Python distributions can be obtained from `numpy.random` or `scipy.stats`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "016acZBqgv9r"
      },
      "source": [
        "In any model, there are multiple sources of fluctuations. One of these is...\n",
        "\n",
        "## Sampling error\n",
        "\n",
        "Asking a human to do more than a 1000 coin tosses might result in that human rebelling. Thus we might be only be able to ask a person to toss 1000 coins. Thus there is some sampling error in their probability estimates. But if we had asked another person to do 1000 tosses, their answers would be somewhat different. These are the fluctuations we talked about, and the error away from the long term tendency, also called the \"population\" tendency, is called sampling error.\n",
        "\n",
        "### Samples from a population of coin flips\n",
        "\n",
        "We'll establish some terminology at first. What we'll do different here is to do a large set of **replications** M, in each of which we did many coin flips, or **observations** N.  We'll call **a single replication a sample of observations**. Thus the number of samples is M, and the sample size is N. \n",
        "\n",
        "![](https://github.com/tech-sohan/Homework/blob/main/images/grid.png?raw=1)\n",
        "\n",
        "Remember that each of these samples have been chosen from a population of size $n >> N$. The 1000 coin tosses from an infinite population so as to not tire the humans.\n",
        "\n",
        "We'll now calculate the mean over the observations in a single sample, or sample mean, for a sample size of 10, with 20 replications. There are thus 20 means.\n",
        "\n",
        "### Q3. Lets write a function to create this code here\n",
        "\n",
        "Create a function `replicate_throws(number_of_samples, sample_size)`. Loop over the samples, throw `sample_size` coin tosses, and calculate the mean of each sample. Return these means. (your returned array will thus be of size `number_of_samples`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27w1Mo7fgv9s"
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlq01AEkgv9s"
      },
      "source": [
        "replicate_throws(number_of_samples=20, sample_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27g4-Z33gv9t"
      },
      "source": [
        "Now lets do 100 samples of 500 coin tosses:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "060J_SqAgv9t"
      },
      "source": [
        "throw_fractions = replicate_throws(number_of_samples=100, sample_size=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xaid3m_gv9u"
      },
      "source": [
        "plt.hist(throw_fractions);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "563HhDbagv9u"
      },
      "source": [
        "As you can see, there are samples with as many as 56% heads in this experiment. \n",
        "\n",
        "What hapens if you increase the sample size?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8oMD9CBgv9v"
      },
      "source": [
        "throw_fractions = replicate_throws(number_of_samples=200, sample_size=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL6S2Mn7gv9w"
      },
      "source": [
        "plt.hist(throw_fractions);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S-iGfPqgv9x"
      },
      "source": [
        "This is a tighter histogram as you might expect! More coin tosses in a sample, less the uncertainty due to sampling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtSQ--FEgv9y"
      },
      "source": [
        "### Q4 multiple replications with multiple sample sizes\n",
        "\n",
        "Let us now do these 200 replications, but this time and store the 200 means for each sample size from 1 to 10001 (in strides of 100, `np.arange(1,10001,100)`) in `sample_means`. This should be a 2D array of 200 rows corresponding to the 200 replications and 100 columns corresponding to the shape of `sample_sizes` below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BE7JpNwgv9y"
      },
      "source": [
        "sample_sizes=np.arange(1,10001,100)\n",
        "sample_sizes.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRC45ycRgv9z"
      },
      "source": [
        "sample_sizes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQJdNuzigv9z"
      },
      "source": [
        "The next cell is yours to write. Its output should be a 200 x 100 array, with rows the means at different sample sizes for different replications. Yoour strategy to fill this 2D array will be to go column by colum and use `replicate_throws` to fill the 200 means for a given sample size in each column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nYBPLIKgv9z"
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhpVvfTLgv90"
      },
      "source": [
        "sample_means.shape # should be 200 x 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJJ_qh9Ugv90"
      },
      "source": [
        "This is now a set of 200 sample means at each sample size. Lets create an array of \"means of sample means\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHmcJ5b3gv90"
      },
      "source": [
        "mean_of_sample_means = np.mean(sample_means, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiPtED38gv91"
      },
      "source": [
        "Now let us plot these:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SY1d23Dgv91"
      },
      "source": [
        "plt.plot(sample_sizes, mean_of_sample_means);\n",
        "plt.ylim([0.480,0.520]);\n",
        "plt.axhline(0.5, 0, 1, color=\"r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvtd0KH_gv91"
      },
      "source": [
        "We can see that the mean of sample means (as opposed to just the sample mean) also gets closer to 0.5. What does the distribution of means look like? Leets see this at the highest sample size (9901), thus the last column in the sample_means array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09DVyg09gv92"
      },
      "source": [
        "plt.hist(sample_means[:,-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17VqVo17gv92"
      },
      "source": [
        "This distribution (in the infinite sample size limit) if called the \"Sampling distribution of the Sample Mean\", as for each replication, we did 9901 (large enough) coin tosses. We calculated the mean of those coin tosses, and plotted the distribution here. The mean of this distribution is, of course:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_NHE7qngv92"
      },
      "source": [
        "mean_of_sample_means[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juqpDgN2gv92"
      },
      "source": [
        "This is the: \n",
        "\n",
        "### The Gaussian Distribution\n",
        "\n",
        "The sampling distribution of the mean itself has a mean $\\mu$ and variance $s^2$. This distribution is called the **Gaussian** or **Normal Distribution**, and is probably the most important distribution in all of statistics.\n",
        "\n",
        "The probability density of the normal distribution is given as:\n",
        "\n",
        "$$ N(x, \\mu, \\sigma) = \\frac{1}{s\\sqrt{2\\pi}} e^{ -\\frac{(x-\\mu)^2}{2s^2} } .$$\n",
        "\n",
        "$s$ is called the **standard error**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbdaxdNmgv93"
      },
      "source": [
        "Veeeeery close to 0.5, as expected. \n",
        "\n",
        "### Q5: Tightening of the distribution\n",
        "\n",
        "But how do these distributions vary at different sample sizes? Plot the distribution of 200 means at 3 different sample sizes: 501, 5001, and 9901 in the same figure. You will need to figure the index corresponding to this (use `zip` and `dict`) and get the three columns out from `sample_means`. Use labels and a legend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3kGWmVPgv93"
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lmP9Nhfgv93"
      },
      "source": [
        "You will notice an immense tightening of the distributions. By how much? To do this lets create an array for the standard deviation of the distributions, also known in statiostical parlance as the **standard error**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_UhysTEgv94"
      },
      "source": [
        "stddev_of_sample_means = np.std(sample_means, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOMKIvzpgv94"
      },
      "source": [
        "### Q6: Plot the log of this array agains the log of the sample sizes.\n",
        "\n",
        "You can use `np.log10`. This function can be applied directly to arrays: this is akin to what we did when we added two arrays. it will apply the function elementwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf8pkbCXgv94"
      },
      "source": [
        "np.log10(stddev_of_sample_means)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k47vZJ6gv94"
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfFdOhmPgv95"
      },
      "source": [
        "Its a straight line. If you look catefully, its slope is -0.5, telling us that:\n",
        "\n",
        "$$Standard Error \\propto \\frac{1}{\\sqrt{N}}$$\n",
        "\n",
        "where $N$ is the sample size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUrYLdhPgv95"
      },
      "source": [
        "This gets us to the:\n",
        "\n",
        "## The Central Limit Theorem\n",
        "\n",
        "The theorem is stated as thus, very similar to the law of large numbers:\n",
        "\n",
        "**Let $x_1,x_2,...,x_n$ be a sequence of independent, identically-distributed (IID) random variables from a random variable $X$. Suppose that $X$ has the finite mean $\\mu$ AND finite variance $\\sigma^2$. Then the average of the first n of them:**\n",
        "\n",
        "$$S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i ,$$\n",
        "\n",
        "**converges to a Gaussian Random Variable with mean $\\mu$ and variance $\\sigma^2/n$ as $n \\to \\infty$:**\n",
        "\n",
        "$$ S_n \\sim N(\\mu,\\frac{\\sigma^2}{n}) \\, as \\, n \\to \\infty. $$\n",
        "\n",
        "In other words:\n",
        "\n",
        "$$s^2 = \\frac{\\sigma^2}{N}.$$\n",
        "\n",
        "\n",
        "This is true, *regardless* of the shape of $X$, which could be binomial, poisson, or any other distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX1_zVSWgv95"
      },
      "source": [
        "Lets step back and try and think about what this all means. As an example, say I have a weight-watchers' study of 1000 people, whose average weight is 150 lbs with standard deviation of 30lbs. If I was to randomly choose many samples of 100 people each, the mean weights of those samples would cluster around 150lbs with a standard error of 30/$\\sqrt{100}$ = 3lbs. Now if i gave you a different sample of 100 people with an average weight of 170lbs, this weight would be more than 6 standard errors beyond the population mean, ^[this example is motivated by the crazy bus example in Charles Whelan's excellent Naked Statistics Book] and would thus be very unlikely to be from the weight watchers group."
      ]
    }
  ]
}